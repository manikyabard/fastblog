{
  
    
        "post0": {
            "title": "AIOps with Spark",
            "content": "Introduction . Everyday people perform various computational processes which demand a lot of online resources throughout the day, month or the year. The amount of data produced per day is massive, and it is important to know the exact usage of resources through this time to make it available to the users as an when required, thereby improving the efficiency. In the following project, we make use of certain dev-ops output data and analyze the variation in the performance throughout the day by performing a time series analysis on the data using spark RDDs. Apache Spark is a unified analytical engine used for big data and machine learning purposes developed at UC Berkeley in 2009. They also founded Databricks from Azure in 2013, which we use here to execute our program . A peek in to the Dataset . A sample of our dataset is shown above. It consists of 12 unique columns: . Index: The basic numbering of all the values from 0 | Provider: The source from where the resources are provided, in this case we use that of Amazon Web Services | Service_Name: The type of service that is being requested for | Account_ID: The account identity from where the request for the resource came. | Region: Represents the regional host/ zone from where the resources are transported | Resource_ID: Unique ID given to every resource sent to the user. | Asset_ID: The unique identity given to the asset that complements the resource. | Timestamp: This shows the time of arrival of dispatched resources. | Value: The amount of it that has been provided. | Unit: All of them are measured in terms of bytes. | Metric_Name: Identifies the metric given to. | Statistic: Denotes how the data was collected, for eg. (count, sum, average etc.) | . df.head() . Unnamed: 0 provider service_name account_id region resource_id asset_id timestamp value unit metric_name statistic . 0 0 | aws | ec2 | 573921696245 | us-east-2 | i-085da62316b03d4cc | 1a4e5a93a9fc21234e56110dfc4569fb | 2020-09-15 07:05:00 | 45.047999 | Percent | CPUUtilization | Average | . 1 1 | aws | ec2 | 573921696245 | us-east-2 | i-085da62316b03d4cc | 1a4e5a93a9fc21234e56110dfc4569fb | 2020-09-15 07:05:00 | 0.000000 | Bytes | DiskReadBytes | Average | . 2 2 | aws | ec2 | 573921696245 | us-east-2 | i-085da62316b03d4cc | 1a4e5a93a9fc21234e56110dfc4569fb | 2020-09-15 07:05:00 | 0.000000 | Bytes | DiskWriteBytes | Average | . 3 3 | aws | ec2 | 573921696245 | us-east-2 | i-085da62316b03d4cc | 1a4e5a93a9fc21234e56110dfc4569fb | 2020-09-15 07:05:00 | 22456.333333 | Bytes | NetworkOut | Average | . 4 4 | aws | ec2 | 573921696245 | us-east-2 | i-085da62316b03d4cc | 1a4e5a93a9fc21234e56110dfc4569fb | 2020-09-15 07:05:00 | 0.000000 | Count | DiskReadOps | Average | . Architecture . . The whole project has two phases: . Training Phase | Production Phase | . In the Training Phase, we read the data from parquet format stored in databricks file system, which is used to train the facebook prophet model. This model is used for forcasting the metric values of different cloud resources. . In the Production Phase, we use structured streaming to get the latest data, and use it with our model. This can be used to further train the model, or make future forecasts. . Inside spark, we train the model, and make plots to visualize the forecasts. . After this, the forecasts are finally stored . Importing required libraries . import numpy as np import pandas as pd from fbprophet import Prophet from pyspark import SparkConf from pyspark.sql import SparkSession from pyspark.sql.functions import collect_list, struct from pyspark.sql.types import FloatType, StructField, StructType, StringType, TimestampType from sklearn.metrics import mean_squared_error from pathlib import Path . Importing plotly failed. Interactive plots will not work. Defining path where data is stored. . path = Path(&quot;/FileStore/tables/&quot;) file_name = &quot;i_0a1c7dc126cb6ac8b_CPUUtilization.csv&quot; . import pyarrow pyarrow.__version__ . Out[2]: &#39;0.14.1&#39; Defining Helper Functions . Retrieves the data from path and cleans it. . def retrieve_data(file_name): &quot;&quot;&quot;Load sample data from ./data/original-input.csv as a pyspark.sql.DataFrame.&quot;&quot;&quot; # df = (spark.read # .option(&quot;header&quot;, &quot;true&quot;) # .option(&quot;inferSchema&quot;, value=True) # .csv(&quot;./data/input.csv&quot;)) df = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, value=True).format(&quot;csv&quot;).load(file_name) # Drop any null values incase they exist df = df.dropna() # Rename timestamp to ds and total to y for fbprophet df = df.select( df[&#39;timestamp&#39;].alias(&#39;ds&#39;), df[&#39;service_name&#39;], df[&#39;value&#39;].cast(FloatType()).alias(&#39;y&#39;), df[&#39;metric_name&#39;] ) return df . Retrieves the data as a stream from path. . def retrieve_datastream(file_name): schema = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, value=True).format(&quot;csv&quot;).load(file_name).schema streamingInputDF = ( spark .readStream .schema(schema) .option(&quot;header&quot;, &quot;true&quot;).format(&quot;csv&quot;).load(file_name)) streamingCountsDF = ( streamingInputDF .groupBy( streamingInputDF.value, window(streamingInputDF.timestamp, &quot;1 hour&quot;)) .count() ) . Function to convert each of the sql rows into a Python dictionary. We append each element to a single dictionalry and convert the final output into a pandas dataframe . def transform_data(row): &quot;&quot;&quot;Transform data from pyspark.sql.Row to python dict to be used in rdd.&quot;&quot;&quot; data = row[&#39;data&#39;] app = row[&#39;service_name&#39;] mt = row[&#39;metric_name&#39;] # Transform [pyspark.sql.Dataframe.Row] -&gt; [dict] data_dicts = [] for d in data: data_dicts.append(d.asDict()) # Convert into pandas dataframe for fbprophet data = pd.DataFrame(data_dicts) data[&#39;ds&#39;] = pd.to_datetime(data[&#39;ds&#39;]) return { &#39;service_name&#39;: app, &#39;metric_name&#39;: mt, &#39;data&#39;: data } . Function used to split the data into training dataset and a testing dataset. The reference used to split would be the value of the timestamp. We initialize a &#39;max_datetime&#39; variable to set a threshold for the split. . def partition_data(d): &quot;&quot;&quot;Split data into training and testing based on timestamp.&quot;&quot;&quot; # Extract data from pd.Dataframe data = d[&#39;data&#39;] # Find max timestamp and extract timestamp for start of day max_datetime = pd.to_datetime(max(data[&#39;ds&#39;])) # start_datetime = max_datetime.replace(hour=00, minute=00, second=00) start_datetime = &#39;10/10/20 00:00&#39; # Extract training data train_data = data[data[&#39;ds&#39;] &lt; start_datetime] # Account for zeros in data while still applying uniform transform # train_data[&#39;y&#39;] = train_data[&#39;y&#39;].apply(lambda x: np.log(x + 1)) # Assign train/test split d[&#39;test_data&#39;] = data.loc[(data[&#39;ds&#39;] &gt;= start_datetime) &amp; (data[&#39;ds&#39;] &lt;= max_datetime)] d[&#39;train_data&#39;] = train_data return d . Function that is used to create a prophet model out if the data we input into it. Prophet takes in certain arguments as shown below and outputs a time series analysis for the same. . def create_model(d): &quot;&quot;&quot;Create Prophet model using each input grid parameter set.&quot;&quot;&quot; m = Prophet() d[&#39;model&#39;] = m return d . Once we have the Prophet model initialized we can fit the training dataset from our previous function and train the model. . def train_model(d): &quot;&quot;&quot;Fit the model using the training data.&quot;&quot;&quot; model = d[&#39;model&#39;] train_data = d[&#39;train_data&#39;] model.fit(train_data) d[&#39;model&#39;] = model return d . Function used to test if the trained model is able to perform smoothly and forecast the right values for the same. . def test_model(d): &quot;&quot;&quot;Run the forecast method on the model to make future predictions.&quot;&quot;&quot; test_data = d[&#39;test_data&#39;] model = d[&#39;model&#39;] t = test_data[&#39;ds&#39;] t = pd.DataFrame(t) t.columns = [&#39;ds&#39;] predictions = model.predict(t) d[&#39;predictions&#39;] = predictions return d . Function to forecast the data values upto a certain defined threshold. . def make_forecast(d): &quot;&quot;&quot;Execute the forecast method on the model to make future predictions.&quot;&quot;&quot; model = d[&#39;model&#39;] future = model.make_future_dataframe( periods=576, freq=&#39;5min&#39;, include_history=False) future = pd.DataFrame(future[&#39;ds&#39;].apply(pd.DateOffset(1))) forecast = model.predict(future) d[&#39;forecast&#39;] = forecast return d . Function used to normalize all the values of our data. . def normalize_predictions(d): &quot;&quot;&quot;Normalize predictions using np.exp().&quot;&quot;&quot; predictions = d[&#39;predictions&#39;] # predictions[&#39;yhat&#39;] = np.exp(predictions[&#39;yhat&#39;]) - 1 d[&#39;predictions&#39;] = predictions return d . Function used to normalize the values of our forecasted data. As mentioned below, since certain values tend to infinity, we replace these values with None using lambda functions. . def normalize_forecast(d): &quot;&quot;&quot;Normalize predictions using np.exp(). Note: np.exp(&gt;709.782) = inf, so replace value with None &quot;&quot;&quot; forecast = d[&#39;forecast&#39;] # forecast[&#39;yhat&#39;] = forecast[&#39;yhat&#39;].apply( # lambda x: np.exp(x) - 1 if x &lt; 709.782 else None) # forecast[&#39;yhat_lower&#39;] = forecast[&#39;yhat_lower&#39;].apply( # lambda x: np.exp(x) - 1 if x &lt; 709.782 else None) # forecast[&#39;yhat_upper&#39;] = forecast[&#39;yhat_upper&#39;].apply( # lambda x: np.exp(x) - 1 if x &lt; 709.782 else None) d[&#39;forecast&#39;] = forecast return d . Function used to calculate the mean squared error of the given test data with respect to the predicted outcomes. . def calc_error(d): &quot;&quot;&quot;Calculate error using mse (mean squared error).&quot;&quot;&quot; test_data = d[&#39;test_data&#39;] predictions = d[&#39;predictions&#39;] results = mean_squared_error(test_data[&#39;y&#39;], predictions[&#39;yhat&#39;]) d[&#39;mse&#39;] = results return d . Function used to return a table of all the previously calculated values of our forecast and our mean square errors. . def reduce_data_scope(d): &quot;&quot;&quot;Return a tuple (service_name + , + metric_type, {}).&quot;&quot;&quot; return ( d[&#39;service_name&#39;] + &#39;,&#39; + d[&#39;metric_name&#39;], { &#39;forecast&#39;: d[&#39;forecast&#39;], &#39;mse&#39;: d[&#39;mse&#39;], }, ) . Function to Flatten rdd into tuple which will be converted into a dataframe.Row. Checks each float to see if it is a np datatype, since it could be None. If it is an np datatype then it will convert to scalar python datatype so that it can be persisted into a database, since most dont know how to interpret np python datatypes. . def expand_predictions(d): &quot;&quot;&quot;Flatten rdd into tuple which will be converted into a dataframe.Row. Checks each float to see if it is a np datatype, since it could be None. If it is an np datatype then it will convert to scalar python datatype so that it can be persisted into a database, since most dont know how to interpret np python datatypes. &quot;&quot;&quot; service_metric, data = d service, metric = service_metric.split(&#39;,&#39;) return [ ( service, metric, p[&#39;ds&#39;].to_pydatetime(), np.asscalar(p[&#39;yhat&#39;]) if isinstance( p[&#39;yhat&#39;], np.generic) else p[&#39;yhat&#39;], np.asscalar(p[&#39;yhat_lower&#39;]) if isinstance( p[&#39;yhat_lower&#39;], np.generic) else p[&#39;yhat_lower&#39;], np.asscalar(p[&#39;yhat_upper&#39;]) if isinstance( p[&#39;yhat_upper&#39;], np.generic) else p[&#39;yhat_upper&#39;], np.asscalar(data[&#39;mse&#39;]) if isinstance( data[&#39;mse&#39;], np.generic) else data[&#39;mse&#39;], ) for i, p in data[&#39;forecast&#39;].iterrows() ] . file_name = &quot;/FileStore/tables/i_0a1c7dc126cb6ac8b_CPUUtilization.csv&quot; . Reading the csv file from the databricks file system (dbfs). . df1 = spark.read.format(&quot;csv&quot;).load(file_name, header=&quot;true&quot;, inferSchema=&quot;true&quot;) . The below function registers a table to make it accessible via SQL content as shown below. . df1.createOrReplaceTempView(&quot;data_ec2&quot;) . %sql select timestamp, value from data_ec2 where metric_name = &#39;CPUUtilization&#39;; . df1 . Out[32]: DataFrame[_c0: int, provider: string, service_name: string, account_id: bigint, region: string, resource_id: string, asset_id: string, timestamp: string, value: double, unit: string, metric_name: string, statistic: string] Training the model . Initializing the spark configuration and spark session. . conf = (SparkConf() .setMaster(&quot;local[*]&quot;) .setAppName(&quot;EC2 Training&quot;)) spark = (SparkSession .builder .config(conf=conf) .getOrCreate()) . sc = spark.sparkContext sc.setLogLevel(&quot;INFO&quot;) . Retrieve data from dbfs . df = retrieve_data(file_name) . Group data by app and metric_type to aggregate data for each app-metric combo. . df = df.groupBy(&#39;service_name&#39;, &#39;metric_name&#39;) df = df.agg(collect_list(struct(&#39;ds&#39;, &#39;y&#39;)).alias(&#39;data&#39;)) . Using the previously defined functions with lambda functions to execute the RDDs and provide our prophet model. . df = (df.rdd .map(lambda r: transform_data(r)) .map(lambda d: partition_data(d)) # prophet cant handle data with &lt; 2 training examples .filter(lambda d: len(d[&#39;train_data&#39;]) &gt; 2) .map(lambda d: create_model(d)) .map(lambda d: train_model(d)) .map(lambda d: test_model(d)) .map(lambda d: make_forecast(d)) .map(lambda d: normalize_forecast(d)) .map(lambda d: normalize_predictions(d)) .map(lambda d: calc_error(d)) .map(lambda d: reduce_data_scope(d)) .flatMap(lambda d: expand_predictions(d))) . Defining the schema of the rdd. . schema = StructType([ StructField(&quot;service_name&quot;, StringType(), True), StructField(&quot;metric_name&quot;, StringType(), True), StructField(&quot;ds&quot;, TimestampType(), True), StructField(&quot;yhat&quot;, FloatType(), True), StructField(&quot;yhat_lower&quot;, FloatType(), True), StructField(&quot;yhat_upper&quot;, FloatType(), True), StructField(&quot;mse&quot;, FloatType(), True) ]) . df = spark.createDataFrame(df, schema) . df . Out[46]: DataFrame[service_name: string, metric_name: string, ds: timestamp, yhat: float, yhat_lower: float, yhat_upper: float, mse: float] %sql select ds, yhat from forecasts . . . df.show(n=5) . . df.createOrReplaceTempView(&quot;forecasts&quot;) . We create a temporary variable to store the pandas dataframe and slice to show the the output. . temp = df.toPandas() . /databricks/spark/python/pyspark/sql/pandas/conversion.py:88: UserWarning: toPandas attempted Arrow optimization because &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true; however, failed by the reason below: PyArrow &gt;= 0.15.1 must be installed; however, your version was 0.14.1. Attempting non-optimization as &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; is set to true. warnings.warn(msg) temp[:30] . service_name metric_name ds yhat yhat_lower yhat_upper mse . 0 ec2 | CPUUtilization | 2020-10-11 00:00:00 | 4.619425 | 2.152688 | 7.399229 | 14.689586 | . 1 ec2 | CPUUtilization | 2020-10-11 00:05:00 | 4.819132 | 2.049825 | 7.313528 | 14.689586 | . 2 ec2 | CPUUtilization | 2020-10-11 00:10:00 | 5.025560 | 2.346648 | 7.663989 | 14.689586 | . 3 ec2 | CPUUtilization | 2020-10-11 00:15:00 | 5.238224 | 2.600470 | 8.062807 | 14.689586 | . 4 ec2 | CPUUtilization | 2020-10-11 00:20:00 | 5.456621 | 2.759909 | 8.302505 | 14.689586 | . 5 ec2 | CPUUtilization | 2020-10-11 00:25:00 | 5.680228 | 3.041043 | 8.384025 | 14.689586 | . 6 ec2 | CPUUtilization | 2020-10-11 00:30:00 | 5.908505 | 3.247892 | 8.670483 | 14.689586 | . 7 ec2 | CPUUtilization | 2020-10-11 00:35:00 | 6.140900 | 3.661118 | 8.664343 | 14.689586 | . 8 ec2 | CPUUtilization | 2020-10-11 00:40:00 | 6.376847 | 3.758875 | 8.888179 | 14.689586 | . 9 ec2 | CPUUtilization | 2020-10-11 00:45:00 | 6.615771 | 3.907142 | 9.169059 | 14.689586 | . 10 ec2 | CPUUtilization | 2020-10-11 00:50:00 | 6.857092 | 4.183938 | 9.378720 | 14.689586 | . 11 ec2 | CPUUtilization | 2020-10-11 00:55:00 | 7.100224 | 4.212289 | 9.687737 | 14.689586 | . 12 ec2 | CPUUtilization | 2020-10-11 01:00:00 | 7.344578 | 4.710319 | 10.216467 | 14.689586 | . 13 ec2 | CPUUtilization | 2020-10-11 01:05:00 | 7.589569 | 5.146057 | 10.505490 | 14.689586 | . 14 ec2 | CPUUtilization | 2020-10-11 01:10:00 | 7.834610 | 5.086944 | 10.616975 | 14.689586 | . 15 ec2 | CPUUtilization | 2020-10-11 01:15:00 | 8.079126 | 5.313694 | 10.592289 | 14.689586 | . 16 ec2 | CPUUtilization | 2020-10-11 01:20:00 | 8.322544 | 5.868225 | 11.002398 | 14.689586 | . 17 ec2 | CPUUtilization | 2020-10-11 01:25:00 | 8.564303 | 5.880727 | 11.178074 | 14.689586 | . 18 ec2 | CPUUtilization | 2020-10-11 01:30:00 | 8.803857 | 6.183046 | 11.552999 | 14.689586 | . 19 ec2 | CPUUtilization | 2020-10-11 01:35:00 | 9.040670 | 6.286131 | 11.792228 | 14.689586 | . 20 ec2 | CPUUtilization | 2020-10-11 01:40:00 | 9.274226 | 6.631486 | 11.928005 | 14.689586 | . 21 ec2 | CPUUtilization | 2020-10-11 01:45:00 | 9.504026 | 6.949830 | 12.192855 | 14.689586 | . 22 ec2 | CPUUtilization | 2020-10-11 01:50:00 | 9.729593 | 7.086974 | 12.337980 | 14.689586 | . 23 ec2 | CPUUtilization | 2020-10-11 01:55:00 | 9.950470 | 7.122755 | 12.677289 | 14.689586 | . 24 ec2 | CPUUtilization | 2020-10-11 02:00:00 | 10.166224 | 7.448644 | 12.650644 | 14.689586 | . 25 ec2 | CPUUtilization | 2020-10-11 02:05:00 | 10.376447 | 7.896529 | 13.116122 | 14.689586 | . 26 ec2 | CPUUtilization | 2020-10-11 02:10:00 | 10.580760 | 7.970045 | 13.270639 | 14.689586 | . 27 ec2 | CPUUtilization | 2020-10-11 02:15:00 | 10.778807 | 8.094746 | 13.579990 | 14.689586 | . 28 ec2 | CPUUtilization | 2020-10-11 02:20:00 | 10.970263 | 8.249172 | 13.512366 | 14.689586 | . 29 ec2 | CPUUtilization | 2020-10-11 02:25:00 | 11.154834 | 8.614250 | 13.950434 | 14.689586 | . Writing the output forecasts to a parquet file stored in dbfs. . df.write.options(header=True).parquet(f&#39;{file_name}_output.parquet&#39;, mode=&#39;overwrite&#39;) . Streaming data . The input path has been defined where the new data gets added. Structured streaming has been performed for testing our model. . from pyspark.sql.types import * from pyspark.sql.functions import * inputPath = &quot;/FileStore/tables/&quot; name = &quot;i_0a1c7dc126cb6ac8b_CPUUtilization.csv&quot; schema = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, value=True).format(&quot;csv&quot;).load(inputPath+name).schema streamingInputDF = ( spark .readStream .schema(schema) .option(&quot;header&quot;, &quot;true&quot;).format(&quot;csv&quot;).load(inputPath+&#39;*.csv&#39;)) streamingCountsDF = ( streamingInputDF .groupBy( streamingInputDF.value, window(streamingInputDF.timestamp, &quot;1 hour&quot;)) .count() ) . query = ( streamingCountsDF .writeStream .format(&quot;memory&quot;) # memory = store in-memory table (for testing only) .queryName(&quot;count&quot;) # counts = name of the in-memory table .outputMode(&quot;complete&quot;) # complete = all the counts should be in the table .start() ) . . %sql select value, window, count from count . .",
            "url": "https://manikyabard.github.io/fastblog/data-analysis/2020/12/09/AIOps-with-Spark.html",
            "relUrl": "/data-analysis/2020/12/09/AIOps-with-Spark.html",
            "date": " • Dec 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Telling Stories with Data - Final Project",
            "content": "Introduction and Background . Here, we will take a look at how different competancy areas relate to each other, and appear together in a person. . These competancy areas are based on CASEL 5. . What is CASEL? . The Collaborative for Academic, Social, and Emotional Learning (CASEL) is a trusted source for knowledge about high-quality, evidence-based social and emotional learning (SEL). . What is SEL? . Social and emotional learning (SEL) is an integral part of education and human development. SEL is the process through which all young people and adults acquire and apply the knowledge, skills, and attitudes to develop healthy identities, manage emotions and achieve personal and collective goals, feel and show empathy for others, establish and maintain supportive relationships, and make responsible and caring decisions. . Benefits of SEL . Learning about this is importance because according to CASEL &quot;Research confirms and teachers, parents, and principals agree: Social and emotional competencies can be taught, modeled, and practiced and lead to positive student outcomes that are important for success in school and in life.&quot; . . Tip: Read more about SEL&#8217;s impact here . About the Data . The data 1 is collected through a survey done which was answered by school and college students, which contained questions related to different aspects of the pillars defined by CASEL, like self awareness, grit, empathy etc. . The columns below refer to the questions in the survey. . display_all(data.head()) . . Age Gender How are you feeling right now? Why? What happened that is affecting your current mood? How many members are there in your household? ( Include Grandparents, Aunts, Uncles, Cousins, Sisters-in-law, Nephews, Nieces, Siblings, etc., if they are living with you or next door to you. example: 2.0) Are you the only child? Are you the oldest child? Are you the youngest child? Do you have any pets? Say, you are playing a fierce game of badminton with your friends. Your mother keeps calling you from the balcony, asking you to come home and have your dinner. What will you do? Approximately how many people, whom you once knew well, do you now avoid, no longer meet, invite, or talk to? (Include anyone you have blocked on your Phone, Whatsapp, Facebook, Instagram, etc.) In the last three years, how many good friends have you made? ( don&#39;t include classmates, acquaintances you are not close to. ) How often have your parents discussed with you, your marks / school work / University subjects? Say during an exam, you momentarily show your answer sheet to a struggling friend. Your teacher decides to punish both of you. You and your friend are asked to bring your parents to School. What would you do in such a situation? How often do you check your social media? &quot;I can easily tell how others are feeling.&quot; nHow strongly do you agree with this statement? Say a popular group of boys and girls from your School or College are angry at your friend. Incidentally you are invited to a house party thrown by this group, but your friend is not invited. What do you usually do in such a situation? &quot;When a best friend is angry, I feel angry too and take my best friend’s side.&quot; nHow strongly do you agree with this statement? Say you move into a new high-rise apartment with a lot of amenities, like gym, swimming pool, clubhouses and you socialize with a few people. You hear gossip against a certain boy called Rohit, stating that he&#39;s not very popular amongst them. What would your approach be pertaining to Rohit? Which of these statements is true? Which of the following statements best describes you the night before the exam? After a failure/setback what do you do? In general, do you ask a lot of questions. How many? ( Googling, asking friends, parents or teachers) You are stuck on some level of competency on a new project. For example, you cannot finish 25 push-ups, find the bug in code, or cross a difficult level on a video game. What do you usually do? Say a friendly scientist from ISRO visits your school and talks about India&#39;s own mars-rover satellite &quot;Mangalyaan&quot;. He invites students to come on stage after the presentation, to look at the robot closely and ask questions one-on-one. However, if you don&#39;t leave before this seminar ends, you will miss the bus back home and will have to walk all the way. What will you do? Are you self-taught in any of these skills? Say you find an error in a study book that your Professor has asked you to refer. What will you do? You overslept and are late for a terrace-top social event. What will you do? &quot;I love public speaking, like in a classroom, Birthday party, Wedding, Team dinner, etc.&quot; n r nHow strongly do you agree with this statement? Would you be friends with yourself? You have just overheard from your mother’s conversation, that your bunch of school friends are planning to throw a surprise birthday party for you, at your home. They will arrive in about 10 minutes. What will you do? Looking at your past incidents, how have you dealt with people who pass insulting personal comments at you? (Say someone makes fun of your looks, or calls you a loser) You get lower than expected grades. You lie to your parents but your sibling (or another relative) catches you and is now threatening to tell your parents. You are facing bullies at School/College Your sibling is permanently leaving home to a far-off place for a job or college, etc. Your pet is missing. Over the last decade have you experienced strong fear that causes panic, shortness of breath, a pounding heart, sweating, shaking, or dizziness? Are you able to control your restless mind from uninvited thoughts that unnecessarily slows you down? How do you describe your physical activity level? Around what time do you go to bed? How do you describe your quality of sleep? Where did you grow up? If the Government can spend on only one of the following, which would you choose? How close are you to nature? What is your view of the world today? You&#39;ve just heard that your younger sibling (or closest friend) has not done well in an important exam. What would you do about this? How often do you laugh at yourself or make jokes on yourself to make others around you laugh or reduce tension in the environment? Does humor come to you spontaneously as a natural part of you? Everyday during commute, you see people violating traffic rules at a particular signal, and the cop there taking bribe and never managing the traffic. You have heard a lot about the entire system being corrupt. All this does not match the India of your dreams, and it saddens you. Looking at your past, what do you think you do in such a situation? When you have a 1-hour homework, how many breaks do you take? Are you generally happy? Are you always on time in reaching school, tuition, party etc? Describe your childhood by choosing 10 words or you can form 5 sentences. . 0 15 | Male | Happy / Excited | Its holiday | 4 | No | Yes | No | False | You affectionately tell your mother, that you ... | 6 | Hardly any | Constantly | Speak to your parents and explain the situatio... | I don&#39;t have social media | Disagree | I will not go and not discuss this incident wi... | Disagree | Meet Rohit to find out what is so bad about him | I&#39;m happy even with a First Class | I stay up late and revise through the night. | I fight harder to reverse the situation, even ... | Very few | I will give myself a break and surely try agai... | You will save the email of the scientist as sh... | Riding bicycle | I will immediately correct the literature with... | I will freshen, do my hair, wear my new dress ... | Neutral | Certainly yes | You tidy up your room, clean the restrooms, f... | It affects me very deeply, I take a long while... | 5 | 1 | 7 | 0 | 0 | Never | Rarely | Low | 11 PM | Average | Tier 2 | Preserving the environment | Frequently close with nature | It is beautiful with many exciting opportuniti... | Lighten their mood by asking them to party wit... | Many | I am usually mischievous, and humor doesn’t le... | approach the cop and appeal to him/her peacefully | A break every 15 minutes | I am generally calm, peaceful and happy all th... | Always on time | I was a friendly and joyful guy | . 1 16 | Female | Calm / Peaceful | Corona | 12 | No | No | No | False | You affectionately tell your mother, that you ... | 0 | Hardly any | Constantly | Speak to your parents and explain the situatio... | Once in three hours | Agree | I will not go and not discuss this incident wi... | Disagree | Take the initiative to help Rohit make friends... | I&#39;m happy even with finishing exams | I sleep early and revise in the morning. | I fight harder to reverse the situation, even ... | Very few | I don’t give up easily. I will push myself unt... | You will stay back to see the satellite at clo... | Riding bicycle | I will ignore my thoughts and trust the experi... | I will freshen, do my hair, wear my new dress ... | Disagree | Certainly yes | You skip freshening up, skip cleaning up the ... | I ignore and avoid such people | 4 | 6 | 9 | 10 | 9 | Never | Sometimes | High | 10 PM | Poor | Tier 2 | Preserving the environment | Sometimes close with nature | It is becoming very selfish, dangerous and hop... | Be with them for as long as they want your com... | A few | Sometimes, I get humorous thoughts and it make... | approach the cop and appeal to him/her peacefully | A break after 30 minutes | I am generally calm, peaceful and happy all th... | Always on time | Beautifully | . 2 18 | Male | Calm / Peaceful | Nothing. | 8 | No | Yes | No | False | You affectionately tell your mother, that you ... | 0 | Some | Often | Speak to your parents and explain the situatio... | Once in hour | Strongly Agree | I will not go and instead spend the evening wi... | Neutral | Ignore the information and treat Rohit like yo... | I&#39;m happy even with a Distinction | I sleep on time, wake up as usual and go for t... | I keep moving. | Many | I don’t give up easily. I will push myself unt... | You will stay back to see the satellite at clo... | Riding bicycle, Cooking | I will Google, ask a friend or reach out to th... | I will leave only after I shower and ensure th... | Strongly Disagree | Certainly yes | You attended to grandpa&#39;s usual call as you wa... | I slam back with equally strong remarks. | 3 | 4 | 5 | 5 | 0 | Rarely | Rarely | Moderate | 1 AM | Good | Metro (Tier 1) | Preserving the environment | Live closely with nature | It is slowly going down | Be with them for as long as they want your com... | A crazy many | I should be a stand-up comedian because I find... | complain about this cop to multiple authoritie... | I finish the homework at a stretch, without an... | I am generally calm, peaceful and happy all th... | Always on time | Is this necessary? 🤔 | . 3 19 | Male | Satisfied | Not much | 4 | No | Yes | No | False | You affectionately tell your mother, that you ... | 0 | Some | Sometimes | Speak to your parents and explain the situatio... | Once in three hours | Neutral | I will go, but I will encourage my friend to c... | Strongly Disagree | Ignore the information and treat Rohit like yo... | I&#39;m happy even with a Distinction | I stay up late and wake up early again to revise. | I keep moving. | A crazy number of questions! | I will give myself a break and surely try agai... | You will miss the bus, and wait for your turn ... | Riding bicycle, Swimming, Ice/Roller skating | I will Google, ask a friend or reach out to th... | I would just put on my shoes and run | Agree | Certainly yes | You attended to grandpa&#39;s usual call as you wa... | I ignore and avoid such people | 8 | 5 | 9 | 4 | 0 | Sometimes | Very often | Low | 2 AM | Poor | Metro (Tier 1) | Make our country modern, hi tech, and have bet... | Sometimes close with nature | Things are better than they used to be. | Be with them for as long as they want your com... | Many | Sometimes, I get humorous thoughts and it make... | I don&#39;t do anything about this | I break a lot and usually take longer than an ... | I feel I am generally calm, peaceful and happy... | Sometimes on time | Calm, learning, privileged, curious, wonderful | . 4 19 | Female | Sad / Down | Life is not just happy | 4 | No | Yes | Yes | True | You affectionately tell your mother, that you ... | 30 | Very few | Often | Ask your friend and his/her parents to support... | Once in hour | Strongly Agree | I will not go and instead spend the evening wi... | Disagree | Ignore the information and treat Rohit like yo... | I&#39;m happy even with a Distinction | I stay up late and revise through the night. | I spend time with people close to me and prefe... | A crazy number of questions! | I will give myself a break and surely try agai... | You will miss the bus, and wait for your turn ... | Cooking, Riding bicycle, Dancing, Playing musi... | I will Google, ask a friend or reach out to th... | I would just put on my shoes and run | Strongly Agree | Certainly yes | You skip freshening up, skip cleaning up the ... | I ignore and avoid such people | 7 | 10 | 5 | 7 | 10 | Very often | Often | High | 12 AM | Average | Tier 2 | Make our country modern, hi tech, and have bet... | Live closely with nature | It is becoming very selfish, dangerous and hop... | Motivate them and tell them that it&#39;s not the ... | A crazy many | I am usually mischievous, and humor doesn’t le... | drive carefully when you pass that signal and ... | I break a lot and usually take longer than an ... | I feel proud at times, but this feeling only l... | Usually on time | It was beautiful | . Further processing was done on this data to make it usable for our models. . Analysis and Insights . Dendrogram . The following plot has been created by using a method called hierarchial clustering on the data. It shows how the answers to the survey questions are related to each other. . The diagram also has an attribute on the left side above each line, which refers to what the question signified. From this, it is possible to see which qualities are related to each other, and appeared together in this data. . . To read this, we need to see which lines are joined to each other. The closer they are joined on the right, the more related they are. . As we can see, the 3rd and 4th line representing &quot;Self Image&quot; and &quot;Intellectual Curiosity&quot; are joined close to each other which means that they are appeared a lot together. . Another insight is that in the middle section with pink colour, we can see that &quot;Anxiety&quot; and &quot;Inner peace&quot; are related to each other, as one would expect. . . Tip: Take a moment to go through the plot and understand it yourself! . . Note: You can read more about Dendrograms here . Clustering Output . The following plot has been created by using a method called k-means clustering on the data after aggregating the data under different categories like &quot;Anxiety&quot;, &quot;Empathy&quot;, &quot;Humour&quot;, etc. (These categories are mentioned on the right legend). . The 4 clusters that were formed shows that in our survey data, these characteristics appeared together. . . We can see in the above plot, people in Cluster 1 had low empathy and compassion, were mostly male (encoded 1 in data), extroverted, and nature lovers amongst other things. . Like this, we can analize all the clusters. . . Tip: Take a moment to go through the plot and understand it yourself! . . Note: You can read more about clustering here . Conclusions . . Warning: There’s a limitation on the front of data that more of it could have been collected with a diverse age range, thus these findings would be a little biased and might apply only to a subset of people. . Footnotes and Extras . 1. The data can be found here.↩ . Some other interesting posts you could check out! . Sign up for CASEL’s free monthly email newsletter to learn about all the latest in the field of #SEL, plus tools and resources. To read the archives and subscribe, go to: https://t.co/VQGIdv095E pic.twitter.com/KAu4WNY58K . &mdash; CASEL (@caselorg) December 4, 2020 . The verdict is in: #SEL Matters! #edchat #edequity #edutwitter @ACT pic.twitter.com/Yojmc2h2Co . &mdash; SocialEmotionalLearning (@SELearningEDU) January 28, 2020 . How do schools, families, &amp; communities work together to create positive outcomes for kids? Hear from CASEL cofounder @RogerWeissberg. Like &amp; share if you agree! pic.twitter.com/4eZtADCll5 . &mdash; CASEL (@caselorg) December 2, 2020 . . .",
            "url": "https://manikyabard.github.io/fastblog/data-analysis/2020/12/04/TSWD_Final.html",
            "relUrl": "/data-analysis/2020/12/04/TSWD_Final.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://manikyabard.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://manikyabard.github.io/fastblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://manikyabard.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://manikyabard.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}